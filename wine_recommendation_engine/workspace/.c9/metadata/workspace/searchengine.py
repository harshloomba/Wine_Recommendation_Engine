{"changed":true,"filter":false,"title":"searchengine.py","tooltip":"/searchengine.py","value":"import bs4\nimport re\nfrom bs4 import *\nimport urllib\nimport sqlite3\nfrom sqlite3 import dbapi2 as sqlite\nignorewords=set(['the','of','to','and','a','in','is','it'])\nclass crawler:\n# Create a list of words to ignore\n \n# Initialize the crawler with the name of database\n def __init__(self,dbname):\n  self.con=sqlite.connect(dbname)\n def __del__(self):\n  self.con.close( )\n def dbcommit(self):\n  self.con.commit( )\n# Auxilliary function for getting an entry id and adding\n# it if it's not present\n def getentryid(self,table,field,value,createnew=True):\n  cur=self.con.execute(\n  \"select rowid from %s where %s='%s'\" % (table,field,value))\n  res=cur.fetchone( )\n  if res==None:\n   cur=self.con.execute(\n   \"insert into %s (%s) values ('%s')\" % (table,field,value))\n   return cur.lastrowid\n  else:\n   return res[0]\n# Index an individual page\n def addtoindex(self,url,soup):\n  if self.isindexed(url): return\n  print ('Indexing '+url)\n  # Get the individual words\n  text=self.gettextonly(soup)\n  words=self.separatewords(text)\n  # Get the URL id\n  urlid=self.getentryid('urllist','url',url)\n  # Link each word to this url\n  for i in range(len(words)):\n   word=words[i]\n   if word in ignorewords: continue\n   wordid=self.getentryid('wordlist','word',word)\n   self.con.execute(\"insert into wordlocation(urlid,wordid,location) \\\n    values (%d,%d,%d)\" % (urlid,wordid,i))\n  \n# Extract the text from an HTML page (no tags)\n def gettextonly(self,soup):\n  v=soup.string\n  if v==None:\n    c=soup.contents \n    resulttext=''\n    for t in c:\n     subtext=self.gettextonly(t)\n     resulttext+=subtext+'\\n'\n    return resulttext\n  else:\n   return v.strip( )\n# Separate the words by any non-whitespace character\n def separatewords(self,text):\n  splitter=re.compile('\\\\W*')\n  return [s.lower( ) for s in splitter.split(text) if s!='']\n# Return true if this url is already indexed\n def isindexed(self,url):\n  u=self.con.execute \\\n    (\"select rowid from urllist where url='%s'\" % url).fetchone( )\n  if u!=None:\n    # Check if it has actually been crawled\n    v=self.con.execute(\n    'select * from wordlocation where urlid=%d' % u[0]).fetchone( )\n    if v!=None: return True\n  return False\n# Add a link between two pages\n def addlinkref(self,urlFrom,urlTo,linkText):\n  pass\n# Starting with a list of pages, do a breadth\n# first search to the given depth, indexing pages\n# as we go\n \n def crawl(self,pages,depth=2):\n  for i in range(depth):\n   newpages=set( )\n   for page in pages:\n     try:\n      c=urlopen(page)\n     except:\n       print ('Could not open %s' % page)\n       continue\n     soup=BeautifulSoup(c.read( ),\"html.parser\")\n     self.addtoindex(page,soup)\n     links=soup('a')\n     for link in links:\n      if ('href' in dict(link.attrs)):\n        url=urljoin(page,link['href'])\n        if url.find(\"'\")!=-1: continue\n        url=url.split('#')[0] # remove location portion\n        if url[0:4]=='http' and not self.isindexed(url):\n          newpages.add(url)\n        linkText=self.gettextonly(link)\n        self.addlinkref(page,url,linkText)\n     self.dbcommit( )\n   pages=newpages\n\n# Create the database tables\n def createindextables(self):\n  self.con.execute('create table urllist(url)')\n  self.con.execute('create table wordlist(word)')\n  self.con.execute('create table wordlocation(urlid,wordid,location)')\n  self.con.execute('create table link(fromid integer,toid integer)')\n  self.con.execute('create table linkwords(wordid,linkid)')\n  self.con.execute('create index wordidx on wordlist(word)')\n  self.con.execute('create index urlidx on urllist(url)')\n  self.con.execute('create index wordurlidx on wordlocation(wordid)')\n  self.con.execute('create index urltoidx on link(toid)')\n  self.con.execute('create index urlfromidx on link(fromid)')\n  self.dbcommit( )\n \n","undoManager":{"mark":49,"position":49,"stack":[[{"start":{"row":0,"column":0},"end":{"row":20,"column":11},"action":"insert","lines":["class crawler:","# Initialize the crawler with the name of database","def __init_ _(self,dbname):","pass","def __del_ _(self):","pass","def dbcommit(self):","pass","# Auxilliary function for getting an entry id and adding","# it if it's not present","def getentryid(self,table,field,value,createnew=True):","return None","# Index an individual page","def addtoindex(self,url,soup):","print 'Indexing %s' % url","# Extract the text from an HTML page (no tags)","def gettextonly(self,soup):","return None","# Separate the words by any non-whitespace character","def separatewords(self,text):","return None"],"id":1}],[{"start":{"row":1,"column":0},"end":{"row":1,"column":1},"action":"insert","lines":[" "],"id":2}],[{"start":{"row":2,"column":0},"end":{"row":2,"column":1},"action":"insert","lines":[" "],"id":3}],[{"start":{"row":3,"column":0},"end":{"row":3,"column":1},"action":"insert","lines":[" "],"id":4}],[{"start":{"row":3,"column":1},"end":{"row":3,"column":2},"action":"insert","lines":[" "],"id":5}],[{"start":{"row":4,"column":0},"end":{"row":4,"column":1},"action":"insert","lines":[" "],"id":6}],[{"start":{"row":5,"column":0},"end":{"row":5,"column":1},"action":"insert","lines":[" "],"id":7}],[{"start":{"row":5,"column":1},"end":{"row":5,"column":2},"action":"insert","lines":[" "],"id":8}],[{"start":{"row":6,"column":0},"end":{"row":6,"column":1},"action":"insert","lines":[" "],"id":9}],[{"start":{"row":7,"column":0},"end":{"row":7,"column":1},"action":"insert","lines":[" "],"id":10}],[{"start":{"row":7,"column":1},"end":{"row":7,"column":2},"action":"insert","lines":[" "],"id":11}],[{"start":{"row":10,"column":0},"end":{"row":10,"column":1},"action":"insert","lines":[" "],"id":12}],[{"start":{"row":11,"column":0},"end":{"row":11,"column":1},"action":"insert","lines":[" "],"id":13}],[{"start":{"row":11,"column":1},"end":{"row":11,"column":2},"action":"insert","lines":[" "],"id":14}],[{"start":{"row":13,"column":0},"end":{"row":13,"column":1},"action":"insert","lines":[" "],"id":15}],[{"start":{"row":14,"column":0},"end":{"row":14,"column":1},"action":"insert","lines":[" "],"id":16}],[{"start":{"row":14,"column":1},"end":{"row":14,"column":2},"action":"insert","lines":[" "],"id":17}],[{"start":{"row":14,"column":2},"end":{"row":14,"column":3},"action":"insert","lines":[" "],"id":18}],[{"start":{"row":14,"column":2},"end":{"row":14,"column":3},"action":"remove","lines":[" "],"id":19}],[{"start":{"row":16,"column":0},"end":{"row":16,"column":1},"action":"insert","lines":[" "],"id":20}],[{"start":{"row":17,"column":0},"end":{"row":17,"column":1},"action":"insert","lines":[" "],"id":21}],[{"start":{"row":17,"column":1},"end":{"row":17,"column":2},"action":"insert","lines":[" "],"id":22}],[{"start":{"row":19,"column":0},"end":{"row":19,"column":1},"action":"insert","lines":[" "],"id":23}],[{"start":{"row":20,"column":0},"end":{"row":20,"column":1},"action":"insert","lines":[" "],"id":24}],[{"start":{"row":20,"column":1},"end":{"row":20,"column":2},"action":"insert","lines":[" "],"id":25}],[{"start":{"row":2,"column":12},"end":{"row":2,"column":13},"action":"remove","lines":[" "],"id":26}],[{"start":{"row":4,"column":11},"end":{"row":4,"column":12},"action":"remove","lines":[" "],"id":27}],[{"start":{"row":20,"column":13},"end":{"row":20,"column":14},"action":"insert","lines":["\\"],"id":28}],[{"start":{"row":20,"column":13},"end":{"row":20,"column":14},"action":"remove","lines":["\\"],"id":29}],[{"start":{"row":20,"column":13},"end":{"row":21,"column":0},"action":"insert","lines":["",""],"id":30},{"start":{"row":21,"column":0},"end":{"row":21,"column":2},"action":"insert","lines":["  "]}],[{"start":{"row":21,"column":2},"end":{"row":34,"column":4},"action":"insert","lines":["# Return true if this url is already indexed","def isindexed(self,url):","return False","# Add a link between two pages","def addlinkref(self,urlFrom,urlTo,linkText):","pass","# Starting with a list of pages, do a breadth","# first search to the given depth, indexing pages","# as we go","def crawl(self,pages,depth=2):","pass","# Create the database tables","def createindextables(self):","pass"],"id":31}],[{"start":{"row":22,"column":0},"end":{"row":22,"column":1},"action":"insert","lines":[" "],"id":32}],[{"start":{"row":23,"column":0},"end":{"row":23,"column":1},"action":"insert","lines":[" "],"id":33}],[{"start":{"row":23,"column":1},"end":{"row":23,"column":2},"action":"insert","lines":[" "],"id":34}],[{"start":{"row":25,"column":0},"end":{"row":25,"column":1},"action":"insert","lines":[" "],"id":35}],[{"start":{"row":26,"column":0},"end":{"row":26,"column":1},"action":"insert","lines":[" "],"id":36}],[{"start":{"row":26,"column":1},"end":{"row":26,"column":2},"action":"insert","lines":[" "],"id":37}],[{"start":{"row":30,"column":0},"end":{"row":30,"column":1},"action":"insert","lines":[" "],"id":38}],[{"start":{"row":31,"column":0},"end":{"row":31,"column":1},"action":"insert","lines":[" "],"id":39}],[{"start":{"row":31,"column":1},"end":{"row":31,"column":2},"action":"insert","lines":[" "],"id":40}],[{"start":{"row":33,"column":0},"end":{"row":33,"column":1},"action":"insert","lines":[" "],"id":41}],[{"start":{"row":34,"column":0},"end":{"row":34,"column":1},"action":"insert","lines":[" "],"id":42}],[{"start":{"row":34,"column":1},"end":{"row":34,"column":2},"action":"insert","lines":[" "],"id":43}],[{"start":{"row":0,"column":0},"end":{"row":34,"column":6},"action":"remove","lines":["class crawler:"," # Initialize the crawler with the name of database"," def __init__(self,dbname):","  pass"," def __del__(self):","  pass"," def dbcommit(self):","  pass","# Auxilliary function for getting an entry id and adding","# it if it's not present"," def getentryid(self,table,field,value,createnew=True):","  return None","# Index an individual page"," def addtoindex(self,url,soup):","  print 'Indexing %s' % url","# Extract the text from an HTML page (no tags)"," def gettextonly(self,soup):","  return None","# Separate the words by any non-whitespace character"," def separatewords(self,text):","  return None","  # Return true if this url is already indexed"," def isindexed(self,url):","  return False","# Add a link between two pages"," def addlinkref(self,urlFrom,urlTo,linkText):","  pass","# Starting with a list of pages, do a breadth","# first search to the given depth, indexing pages","# as we go"," def crawl(self,pages,depth=2):","  pass","# Create the database tables"," def createindextables(self):","  pass"],"id":44},{"start":{"row":0,"column":0},"end":{"row":58,"column":0},"action":"insert","lines":["class crawler:","# Initialize the crawler with the name of database"," def __init__(self,dbname):","  pass"," def __del__(self):","  pass"," def dbcommit(self):","  pass","# Auxilliary function for getting an entry id and adding","# it if it's not present"," def getentryid(self,table,field,value,createnew=True):","  return None","# Index an individual page"," def addtoindex(self,url,soup):","  print 'Indexing %s' % url","# Extract the text from an HTML page (no tags)"," def gettextonly(self,soup):","  return None","# Separate the words by any non-whitespace character"," def separatewords(self,text):","  return None","# Return true if this url is already indexed"," def isindexed(self,url):","  return False","# Add a link between two pages"," def addlinkref(self,urlFrom,urlTo,linkText):","  pass","# Starting with a list of pages, do a breadth","# first search to the given depth, indexing pages","# as we go"," def crawl(self,pages,depth=2):","  pass","# Create the database tables"," def createindextables(self):","  pass"," def crawl(self,pages,depth=2):","  for i in range(depth):","   newpages=set( )","    for page in pages:","     try:","       c=urllib2.urlopen(page)","     except:","       print \"Could not open %s\" % page","       continue","     soup=BeautifulSoup(c.read( ))","     self.addtoindex(page,soup)","     links=soup('a')","     for link in links:","      if ('href' in dict(link.attrs)):","        url=urljoin(page,link['href'])","        if url.find(\"'\")!=-1: continue","        url=url.split('#')[0] # remove location portion","        if url[0:4]=='http' and not self.isindexed(url):","          newpages.add(url)","        linkText=self.gettextonly(link)","        self.addlinkref(page,url,linkText)","     self.dbcommit( )","   pages=newpages",""]}],[{"start":{"row":38,"column":3},"end":{"row":38,"column":4},"action":"remove","lines":[" "],"id":45}],[{"start":{"row":30,"column":1},"end":{"row":31,"column":6},"action":"remove","lines":["def crawl(self,pages,depth=2):","  pass"],"id":46}],[{"start":{"row":39,"column":6},"end":{"row":39,"column":7},"action":"remove","lines":[" "],"id":47}],[{"start":{"row":39,"column":14},"end":{"row":39,"column":15},"action":"remove","lines":["2"],"id":48}],[{"start":{"row":0,"column":0},"end":{"row":57,"column":0},"action":"remove","lines":["class crawler:","# Initialize the crawler with the name of database"," def __init__(self,dbname):","  pass"," def __del__(self):","  pass"," def dbcommit(self):","  pass","# Auxilliary function for getting an entry id and adding","# it if it's not present"," def getentryid(self,table,field,value,createnew=True):","  return None","# Index an individual page"," def addtoindex(self,url,soup):","  print 'Indexing %s' % url","# Extract the text from an HTML page (no tags)"," def gettextonly(self,soup):","  return None","# Separate the words by any non-whitespace character"," def separatewords(self,text):","  return None","# Return true if this url is already indexed"," def isindexed(self,url):","  return False","# Add a link between two pages"," def addlinkref(self,urlFrom,urlTo,linkText):","  pass","# Starting with a list of pages, do a breadth","# first search to the given depth, indexing pages","# as we go"," ","# Create the database tables"," def createindextables(self):","  pass"," def crawl(self,pages,depth=2):","  for i in range(depth):","   newpages=set( )","   for page in pages:","     try:","      c=urllib.urlopen(page)","     except:","       print \"Could not open %s\" % page","       continue","     soup=BeautifulSoup(c.read( ))","     self.addtoindex(page,soup)","     links=soup('a')","     for link in links:","      if ('href' in dict(link.attrs)):","        url=urljoin(page,link['href'])","        if url.find(\"'\")!=-1: continue","        url=url.split('#')[0] # remove location portion","        if url[0:4]=='http' and not self.isindexed(url):","          newpages.add(url)","        linkText=self.gettextonly(link)","        self.addlinkref(page,url,linkText)","     self.dbcommit( )","   pages=newpages",""],"id":49},{"start":{"row":0,"column":0},"end":{"row":120,"column":0},"action":"insert","lines":["import bs4","import re","from bs4 import *","import urllib","from urllib.parse import urlparse","from urllib.parse import urljoin","from urllib.request import urlopen","import sqlite3","from sqlite3 import dbapi2 as sqlite","ignorewords=set(['the','of','to','and','a','in','is','it'])","class crawler:","# Create a list of words to ignore"," ","# Initialize the crawler with the name of database"," def __init__(self,dbname):","  self.con=sqlite.connect(dbname)"," def __del__(self):","  self.con.close( )"," def dbcommit(self):","  self.con.commit( )","# Auxilliary function for getting an entry id and adding","# it if it's not present"," def getentryid(self,table,field,value,createnew=True):","  cur=self.con.execute(","  \"select rowid from %s where %s='%s'\" % (table,field,value))","  res=cur.fetchone( )","  if res==None:","   cur=self.con.execute(","   \"insert into %s (%s) values ('%s')\" % (table,field,value))","   return cur.lastrowid","  else:","   return res[0]","# Index an individual page"," def addtoindex(self,url,soup):","  if self.isindexed(url): return","  print ('Indexing '+url)","  # Get the individual words","  text=self.gettextonly(soup)","  words=self.separatewords(text)","  # Get the URL id","  urlid=self.getentryid('urllist','url',url)","  # Link each word to this url","  for i in range(len(words)):","   word=words[i]","   if word in ignorewords: continue","   wordid=self.getentryid('wordlist','word',word)","   self.con.execute(\"insert into wordlocation(urlid,wordid,location) \\","    values (%d,%d,%d)\" % (urlid,wordid,i))","  ","# Extract the text from an HTML page (no tags)"," def gettextonly(self,soup):","  v=soup.string","  if v==None:","    c=soup.contents ","    resulttext=''","    for t in c:","     subtext=self.gettextonly(t)","     resulttext+=subtext+'\\n'","    return resulttext","  else:","   return v.strip( )","# Separate the words by any non-whitespace character"," def separatewords(self,text):","  splitter=re.compile('\\\\W*')","  return [s.lower( ) for s in splitter.split(text) if s!='']","# Return true if this url is already indexed"," def isindexed(self,url):","  u=self.con.execute \\","    (\"select rowid from urllist where url='%s'\" % url).fetchone( )","  if u!=None:","    # Check if it has actually been crawled","    v=self.con.execute(","    'select * from wordlocation where urlid=%d' % u[0]).fetchone( )","    if v!=None: return True","  return False","# Add a link between two pages"," def addlinkref(self,urlFrom,urlTo,linkText):","  pass","# Starting with a list of pages, do a breadth","# first search to the given depth, indexing pages","# as we go"," "," def crawl(self,pages,depth=2):","  for i in range(depth):","   newpages=set( )","   for page in pages:","     try:","      c=urlopen(page)","     except:","       print ('Could not open %s' % page)","       continue","     soup=BeautifulSoup(c.read( ),\"html.parser\")","     self.addtoindex(page,soup)","     links=soup('a')","     for link in links:","      if ('href' in dict(link.attrs)):","        url=urljoin(page,link['href'])","        if url.find(\"'\")!=-1: continue","        url=url.split('#')[0] # remove location portion","        if url[0:4]=='http' and not self.isindexed(url):","          newpages.add(url)","        linkText=self.gettextonly(link)","        self.addlinkref(page,url,linkText)","     self.dbcommit( )","   pages=newpages","","# Create the database tables"," def createindextables(self):","  self.con.execute('create table urllist(url)')","  self.con.execute('create table wordlist(word)')","  self.con.execute('create table wordlocation(urlid,wordid,location)')","  self.con.execute('create table link(fromid integer,toid integer)')","  self.con.execute('create table linkwords(wordid,linkid)')","  self.con.execute('create index wordidx on wordlist(word)')","  self.con.execute('create index urlidx on urllist(url)')","  self.con.execute('create index wordurlidx on wordlocation(wordid)')","  self.con.execute('create index urltoidx on link(toid)')","  self.con.execute('create index urlfromidx on link(fromid)')","  self.dbcommit( )"," ",""]}],[{"start":{"row":4,"column":0},"end":{"row":7,"column":0},"action":"remove","lines":["from urllib.parse import urlparse","from urllib.parse import urljoin","from urllib.request import urlopen",""],"id":50}]]},"ace":{"folds":[],"scrolltop":0,"scrollleft":0,"selection":{"start":{"row":13,"column":19},"end":{"row":13,"column":19},"isBackwards":true},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":0},"timestamp":1437354537000}